{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础任务（必做）\n",
    "\n",
    ">使用 Python 统计这篇[文章](./happiness_seg.txt)出现频率最高的前 10 个「二元词组」，并输出它们的频率。「二元词组」即文章中所有接连出现的两个词，如「今天 天气 不错」有「今天 天气」，「天气 不错」两个「二元词组」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这篇文章其实是一本书的节选，罗素的《幸福之路》。已经完成了**分词**，所以只需要像对英文单词一样统计二元词组（[bigrams](https://en.wikipedia.org/wiki/Bigram)）即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python处理bigrams，很容易就找到了[nltk库](http://www.nltk.org/)，作业代码主要参考了nltk库关于[Collocations](http://www.nltk.org/howto/collocations.html)的介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取txt文件，并转换成utf-8格式，便于后续处理\n",
    "# 参考知乎这个问题：https://www.zhihu.com/question/20922994\n",
    "text = open(\"./happiness_seg.txt\").read()\n",
    "text_utf8 = text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将分好词的text_utf8转换为列表(list)\n",
    "# tokenization: break up the string into words and punctuation\n",
    "tokens = nltk.word_tokenize(text_utf8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找到tokens中所有的bigrams\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finder.ngram_fd返回一个FreqDist，里面记录了所有的bigrams及其出现次数；most_common(n=None)按由大到小的列出出现频率最高的n个\n",
    "freq_bigrams = finder.ngram_fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的 。 969\n",
      "的 人 923\n",
      "的 ， 919\n",
      "， 而 703\n",
      "。 在 656\n",
      "， 他 638\n",
      "， 在 564\n",
      "， 但 518\n",
      "他 的 503\n",
      "， 他们 501\n",
      "， 因为 484\n",
      "自己 的 480\n",
      "了 。 472\n",
      "。 我 459\n",
      "， 我 377\n",
      "人 ， 373\n",
      "时 ， 368\n",
      "上 的 356\n",
      "他们 的 335\n",
      "， 我们 330\n"
     ]
    }
   ],
   "source": [
    "# 打印出前20个最常出现的bigrams\n",
    "for i in freq_bigrams[:20]:\n",
    "    print i[0][0], i[0][1], i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，`nltk`在统计bigrams时，将中文标点也作为一个词进行统计，这是我们不希望看到的，我们需要使用`apply_word_filter`方法将`finder`中包含中文标点的bigrams过滤掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 过滤掉包含中文标点的bigrams（这里仅列出前20个bigrams中出现的中文标点）\n",
    "finder.apply_word_filter(lambda w: w in [u\"，\", u\"。\", u\"―\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的 人 923\n",
      "他 的 503\n",
      "自己 的 480\n",
      "上 的 356\n",
      "他们 的 335\n",
      "人 的 293\n",
      "的 时候 261\n",
      "就 会 225\n",
      "的 东西 207\n",
      "都 是 206\n",
      "的 生活 193\n",
      "的 孩子 192\n",
      "一个 人 191\n",
      "的 是 184\n",
      "中 的 176\n",
      "我们 的 170\n",
      "也 是 163\n",
      "是 一种 162\n",
      "并 不 150\n",
      "幸福 的 150\n"
     ]
    }
   ],
   "source": [
    "# finder.ngram_fd返回一个FreqDist，里面记录了所有的bigrams及其出现次数；most_common(n=None)按由大到小的列出出现频率最高的n个\n",
    "freq_bigrams = finder.ngram_fd.most_common()\n",
    "# 打印出前20个最常出现的bigrams\n",
    "for i in freq_bigrams[:20]:\n",
    "    print i[0][0], i[0][1], i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "1. 对整个`nltk`的处理流程不熟悉，之前在各种地方东看一点西看一点，还不如将官方文档中关于[Collocations](http://www.nltk.org/howto/collocations.html)的介绍仔细读几遍。\n",
    "2. 对于bigrams的理解：比如对于`\"我 羡慕 那些 已死 的 人 ， 他们 比 活着 的 人 幸福 多 了 。\"`这个句子，直接处理的话会得到`[\"人\", \"，\"]`与`[\"，\", \"他们\"]`这样包含标点的bigrams，我最开始的想法的是用`RegexpTokenizer`将中文标点过滤掉，然后再进行处理，这样得到的结果也OK，但有一个问题，就是它会将`\"人 他们\"`也作为一个bigram，这是不对的。最后，我在找到的bigrams中手动过滤掉包含中文标点的那些，也就是上面得到的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶任务\n",
    "\n",
    "> 1. 求解以下函数相对 x 的导数：$\\sin(x^2)x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算过程\n",
    "\n",
    "$\\frac{d[\\sin(x^2)x]}{dx} = \\frac{d\\sin(x^2)}{dx} * x + \\frac{dx}{dx} * \\sin(x^2)$\n",
    "\n",
    "$                          = \\cos(x^2) * \\frac{d(x^2)}{dx} * x+ \\sin(x^2)$\n",
    "\n",
    "$                          = 2x^2\\cos(x^2) + \\sin(x^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sympy`验算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2*x**2*cos(x**2) + sin(x**2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x=Symbol(\"x\")\n",
    "diff(sin(x**2) * x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. 向量化 (vectorization) 计算是科学计算中必备的技巧。调研 Python 的 numpy 库中「向量化」的概念。熟悉概念之后，使用标准正态分布生成一个 100 元素的数组（以 0 为均值），用向量化方法将这个数组中 < 0 的元素设置成 0 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建均值为0、标准差为1的数组（100个元素）\n",
    "arr = np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.13847647,  1.3185833 ,  0.40951774, -0.23678245,  0.95333865,\n",
       "        0.01795643,  1.10067278,  0.41932356, -1.54630648,  1.84876517,\n",
       "        0.45745092,  0.34099997, -0.95449609,  0.28493364, -1.565713  ,\n",
       "       -0.38414056,  1.30743015,  2.53462636,  1.54849468, -1.01168673,\n",
       "        0.55652856, -2.81996272,  0.68578736,  0.06256857, -0.37514988,\n",
       "        0.90784992, -0.05395024, -0.87982884, -1.69481728,  0.09768135,\n",
       "       -1.52727915, -0.1284582 ,  0.42420991,  0.41925996, -0.9227357 ,\n",
       "       -0.36758143,  0.66211591, -1.08306497,  0.45875461, -0.68182798,\n",
       "       -0.72027627,  1.46871355, -0.68537937,  0.77164654, -1.42362615,\n",
       "       -0.18351451, -0.63706246, -0.23325078, -1.09297405,  1.36993955,\n",
       "       -0.60342532, -0.37179219,  0.66212395, -1.90544243,  0.15389504,\n",
       "        0.78976735,  1.11291208, -0.1505949 ,  0.01370636,  0.51633965,\n",
       "        0.44980089,  0.07520674,  0.01036546, -0.31408122, -0.68124176,\n",
       "        0.49401941, -1.68838863, -2.07009611, -0.05011553,  1.58422674,\n",
       "       -0.43368575, -0.70051318,  0.14731727, -0.05983095, -0.76744022,\n",
       "       -0.85794155, -0.12407443,  0.06220563, -0.31327576, -2.02844002,\n",
       "        1.00598857, -0.12859964, -2.50262564,  0.93659231, -1.13937869,\n",
       "        0.72890554,  0.3771829 , -0.47127238, -0.04527612,  0.1211995 ,\n",
       "        1.25829194, -1.39989574, -1.62085113, -0.98922521,  0.94357675,\n",
       "       -0.65085441,  0.38855124,  0.25893747, -0.1763227 ,  0.59319116])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将arr中小于0的元素设为0\n",
    "arr[arr < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  1.3185833 ,  0.40951774,  0.        ,  0.95333865,\n",
       "        0.01795643,  1.10067278,  0.41932356,  0.        ,  1.84876517,\n",
       "        0.45745092,  0.34099997,  0.        ,  0.28493364,  0.        ,\n",
       "        0.        ,  1.30743015,  2.53462636,  1.54849468,  0.        ,\n",
       "        0.55652856,  0.        ,  0.68578736,  0.06256857,  0.        ,\n",
       "        0.90784992,  0.        ,  0.        ,  0.        ,  0.09768135,\n",
       "        0.        ,  0.        ,  0.42420991,  0.41925996,  0.        ,\n",
       "        0.        ,  0.66211591,  0.        ,  0.45875461,  0.        ,\n",
       "        0.        ,  1.46871355,  0.        ,  0.77164654,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.36993955,\n",
       "        0.        ,  0.        ,  0.66212395,  0.        ,  0.15389504,\n",
       "        0.78976735,  1.11291208,  0.        ,  0.01370636,  0.51633965,\n",
       "        0.44980089,  0.07520674,  0.01036546,  0.        ,  0.        ,\n",
       "        0.49401941,  0.        ,  0.        ,  0.        ,  1.58422674,\n",
       "        0.        ,  0.        ,  0.14731727,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.06220563,  0.        ,  0.        ,\n",
       "        1.00598857,  0.        ,  0.        ,  0.93659231,  0.        ,\n",
       "        0.72890554,  0.3771829 ,  0.        ,  0.        ,  0.1211995 ,\n",
       "        1.25829194,  0.        ,  0.        ,  0.        ,  0.94357675,\n",
       "        0.        ,  0.38855124,  0.25893747,  0.        ,  0.59319116])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
